{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "I chose 2 web pages:\n",
    "    1) https://improvado.io/customer/healthcare-software-provider \n",
    "    2) https://improvado.io/customer/hyperconnect\n",
    "The Healthcare softwre page I chose as a test and the latter was my train dataset."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Introduction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There is an initial draft version in the end as well. \n",
    "Although the result was not so great, it served as a foundation for my final version. \n",
    "There were a lot of experimenting which I decided not to include here.\n",
    "Hope my work is clear and readable. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Final version"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Found cached dataset csv (C:/Users/Aigerim/.cache/huggingface/datasets/csv/data-726cb969e4b012c5/0.0.0/6954658bab30a358235fa864b05cf819af0e179325c740e4bc853bcc7ec513e1)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0345d26b9f5c4e4ab207409c3e621a09",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import torch\n",
    "from datasets import load_dataset\n",
    "from transformers.models.bert import BertTokenizer, BertForQuestionAnswering\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "\n",
    "datafiles = {\"train\": \"datatrain.csv\", \"test\": \"datatest.csv\"}\n",
    "dataset = load_dataset(\"data\", data_files=datafiles, delimiter=\";\", encoding='cp1252')\n",
    "\n",
    "def segment_documents(dataset, max_doc_length=450):\n",
    "    segmented_docs = []\n",
    "    for i in range(len(dataset)):\n",
    "        doc_values = list(dataset[i].values())\n",
    "        segmented_docs.extend([' '.join(doc_values[j:j+max_doc_length]) for j in range(0, len(doc_values), max_doc_length)])\n",
    "    return segmented_docs\n",
    "\n",
    "def get_top_k_articles(query, segmented_docs, k=2):\n",
    "    vectorizer = TfidfVectorizer(analyzer=\"word\", stop_words='english')\n",
    "    query_and_docs = [query] + segmented_docs\n",
    "    matrix = vectorizer.fit_transform(query_and_docs)\n",
    "    scores = [cosine_similarity(matrix[0], matrix[i])[0][0] for i in range(1, len(query_and_docs))]\n",
    "    sorted_list = sorted(enumerate(scores), key=lambda x: x[1], reverse=True)\n",
    "    top_doc_indices = [x[0] for x in sorted_list[:k]]\n",
    "    top_docs = [segmented_docs[x] for x in top_doc_indices]\n",
    "    return top_docs\n",
    "\n",
    "model = BertForQuestionAnswering.from_pretrained('bert-large-uncased-whole-word-masking-finetuned-squad')\n",
    "tokenizer = BertTokenizer.from_pretrained('bert-large-uncased-whole-word-masking-finetuned-squad')\n",
    "\n",
    "def answer_question(question, answer_text):\n",
    "    input_ids = tokenizer.encode(question, answer_text, max_length=512, return_tensors='pt')\n",
    "    token_type_ids = torch.zeros_like(input_ids)\n",
    "    attention_mask = torch.ones_like(input_ids)\n",
    "\n",
    "    outputs = model(input_ids, token_type_ids=token_type_ids, attention_mask=attention_mask, return_dict=True)\n",
    "\n",
    "    start_scores = outputs.start_logits\n",
    "    end_scores = outputs.end_logits\n",
    "\n",
    "    answer_start = torch.argmax(start_scores)\n",
    "    answer_end = torch.argmax(end_scores) + 1\n",
    "\n",
    "    # Convert the tokens back to text\n",
    "    answer_tokens = tokenizer.convert_ids_to_tokens(input_ids[0][answer_start:answer_end])\n",
    "    answer = tokenizer.convert_tokens_to_string(answer_tokens)\n",
    "\n",
    "    print('Answer: \"' + answer + '\" with ' + f'{torch.max(torch.softmax(start_scores, dim=1)).item()*100:.2f}' + '% confidence' )\n",
    "    return answer\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tests"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Answer: \"employees across the company\" with 9.03% confidence\n",
      "Reference Document:  Our solution helped Hyperconnect improve their marketing performance indicators and reduce employees' time on routine data operations. Now, our client has access to more granular data and makes informed decisions faster. According to Hyperconnect, their marketing team now concentrates on marketing campaign optimization rather than data integration tasks. Furthermore, our ETL system provided employees across the company with easy access to all marketing reports. Sales managers, stakeholders, business analysts, and other competent employees can now analyze the company’s marketing efforts whenever they need to and instantly draw up marketing reports. Our advanced data extraction system has helped the company’s analysts find previously overlooked data and more precisely predict the outcomes of marketing strategies.  We’ve also provided Hyperconnect with more flexibility. Improvado is a scalable system, so when our client needs to add new marketing sources, we’ll be able to do it in a matter of seconds.\n",
      "Answer: \"400\" with 62.90% confidence\n",
      "Reference Document:  Hyperconnect aims to empower each person with an ability to connect and keep in touch with others. It’s a mid-sized company with around 400 employees, headquartered in Seoul, South Korea. As a global social platform, the company provides video and AI-powered software that helps users communicate in real time.\n",
      "Hyperconnect was dealing with fragmented data across different marketing channels. To extract this data, the company allocated developers to write lines of code that acted as marketing connectors. This process took too much time and led to  excessive use of the company’s resources. Furthermore, the gathered data was not normalized, which prevented marketing analysts from making informed decisions. Eventually, Hyperconnect contacted Improvado to optimize the data extraction process.\n",
      "Answer: \"\" with 9.80% confidence\n",
      "Reference Document:  Hyperconnect didn’t consider any solutions other than Improvado. Having found many positive reviews, the company’s HQ contacted us to solve their data extraction issues. During this collaboration, they complimented the high service level and the quality of our product. Hyperconnect described Improvado as “A Swiss army knife for marketing data.” Here’s what our client says about Improvado's influence on their marketing processes: “I believe we can now concentrate on more important tasks, such as optimization of the marketing campaigns, rather than data integration tasks.” We’ve helped Hyperconnect to significantly reduce their efforts on manual data operations and shortened the process of extracting new data.\n",
      "------------------------------------\n"
     ]
    }
   ],
   "source": [
    "# Enter our query here\n",
    "import torch \n",
    "query = \"How many employees in Hyperconnect company?\"\n",
    "#query = \"What else does the bassist for Death From Above play?\"\n",
    "#query = \"What projects is Jesse Keeler involved in?\"\n",
    "\n",
    "# Segment our documents\n",
    "segmented_docs = segment_documents(dataset['train'], 450)\n",
    "\n",
    "# Retrieve the top k most relevant documents to the query\n",
    "candidate_docs = get_top_k_articles(query, segmented_docs, 3)\n",
    "\n",
    "# Return the likeliest answers from each of our top k most relevant documents in descending order\n",
    "for i in candidate_docs:\n",
    "    answer_question(query, i)\n",
    "    print (\"Reference Document: \", i)\n",
    "    \n",
    "print(\"------------------------------------\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Answer: \"seoul , south korea\" with 98.32% confidence\n",
      "Reference Document:  Hyperconnect aims to empower each person with an ability to connect and keep in touch with others. It’s a mid-sized company with around 400 employees, headquartered in Seoul, South Korea. As a global social platform, the company provides video and AI-powered software that helps users communicate in real time.\n",
      "Hyperconnect was dealing with fragmented data across different marketing channels. To extract this data, the company allocated developers to write lines of code that acted as marketing connectors. This process took too much time and led to  excessive use of the company’s resources. Furthermore, the gathered data was not normalized, which prevented marketing analysts from making informed decisions. Eventually, Hyperconnect contacted Improvado to optimize the data extraction process.\n",
      "Answer: \"\" with 14.76% confidence\n",
      "Reference Document:  Our solution helped Hyperconnect improve their marketing performance indicators and reduce employees' time on routine data operations. Now, our client has access to more granular data and makes informed decisions faster. According to Hyperconnect, their marketing team now concentrates on marketing campaign optimization rather than data integration tasks. Furthermore, our ETL system provided employees across the company with easy access to all marketing reports. Sales managers, stakeholders, business analysts, and other competent employees can now analyze the company’s marketing efforts whenever they need to and instantly draw up marketing reports. Our advanced data extraction system has helped the company’s analysts find previously overlooked data and more precisely predict the outcomes of marketing strategies.  We’ve also provided Hyperconnect with more flexibility. Improvado is a scalable system, so when our client needs to add new marketing sources, we’ll be able to do it in a matter of seconds.\n",
      "Answer: \"hq\" with 37.03% confidence\n",
      "Reference Document:  Hyperconnect didn’t consider any solutions other than Improvado. Having found many positive reviews, the company’s HQ contacted us to solve their data extraction issues. During this collaboration, they complimented the high service level and the quality of our product. Hyperconnect described Improvado as “A Swiss army knife for marketing data.” Here’s what our client says about Improvado's influence on their marketing processes: “I believe we can now concentrate on more important tasks, such as optimization of the marketing campaigns, rather than data integration tasks.” We’ve helped Hyperconnect to significantly reduce their efforts on manual data operations and shortened the process of extracting new data.\n"
     ]
    }
   ],
   "source": [
    "# Enter our query here\n",
    "import torch \n",
    "# query = \"How many employees in Hyperconnect company?\"\n",
    "query = \"Where is located headquater of Hyperconnect company?\"\n",
    "#query = \"What projects is Jesse Keeler involved in?\"\n",
    "\n",
    "# Segment our documents\n",
    "segmented_docs = segment_documents(dataset['train'], 450)\n",
    "\n",
    "# Retrieve the top k most relevant documents to the query\n",
    "candidate_docs = get_top_k_articles(query, segmented_docs, 3)\n",
    "\n",
    "# Return the likeliest answers from each of our top k most relevant documents in descending order\n",
    "for i in candidate_docs:\n",
    "    answer_question(query, i)\n",
    "    print (\"Reference Document: \", i)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluation"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "I did couple of test (though I need to do a lot more obv) and all of them showed great results. \n",
    "For the 1st question system gave a correct answer with over 60% of confidence. \n",
    "Whereas for the second question, the system was correct with 98% of confidence.\n",
    "\n",
    "Comparing with the result that I had in the beginning, the final version is far superior.\n",
    "correct (+/-)\n",
    "|             |   Question 1    |   Question 2    |\n",
    "---------------------------------------------------\n",
    "|final_version|   Right (62.90%)|   Right (98.32%)|\n",
    "|draft version|   Wrong (84%)   |   Right (55%)   |"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A good amount of improvements needs to be done.\n",
    "1) make a dataset with questions and answers to each questions\n",
    "2) add metrics (F1, Recall, prediction) so it would be easier to visualize\n",
    "3) Add tuning or maybe use the trained model to apply the model to test dataset\n",
    "4) Use other models such as GPT2, GPT3 and so on.\n",
    "5) Build interface around this system. With website link and question as an input and Answer and paragraph as an output.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "P.S. I really enjoyed this, thank you! "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Drafts"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Draft 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Found cached dataset csv (C:/Users/Aigerim/.cache/huggingface/datasets/csv/data-726cb969e4b012c5/0.0.0/6954658bab30a358235fa864b05cf819af0e179325c740e4bc853bcc7ec513e1)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e71e7086a6ae44b6a86a5b7e00c1a6e8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import torch\n",
    "from datasets import load_dataset\n",
    "from ast import literal_eval\n",
    "datafiles = {\"train\": \"datatrain.csv\", \"test\": \"datatest.csv\"}\n",
    "dataset = load_dataset(\"data\", data_files = datafiles, delimiter=\";\", encoding='cp1252')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Marketing data organization is a huge problem when promoting your product across different channels and countries and managing multiple campaigns. Our client, Hyperconnect, faced this problem and asked us to find a solution.\n",
      "Hyperconnect aims to empower each person with an ability to connect and keep in touch with others. It’s a mid-sized company with around 400 employees, headquartered in Seoul, South Korea. As a global social platform, the company provides video and AI-powered software that helps users communicate in real time.\r\n",
      "Hyperconnect was dealing with fragmented data across different marketing channels. To extract this data, the company allocated developers to write lines of code that acted as marketing connectors. This process took too much time and led to  excessive use of the company’s resources. Furthermore, the gathered data was not normalized, which prevented marketing analysts from making informed decisions. Eventually, Hyperconnect contacted Improvado to optimize the data extraction process.\n",
      "Hyperconnect’s main marketing focus was on Facebook Ads, Google Ads, Snapchat, and Twitter. To extract data from these sources, the company’s development team had to write code each time they needed to invoke the API. This approach negatively impacted cross-channel marketing analysis, slowed down the marketing team, and provided inaccurate data. Furthermore, Hyperconnect’s data extraction approach made it difficult to store and arrange data inside the warehouse. Monitoring metrics and campaign statuses was one of the main objectives of the marketing department. That’s why it was crucial to have detailed data for marketing campaign optimization and adjustment of the client’s budgets and bidding strategies. Before our collaboration, Hyperconnect already had an external data warehouse for their marketing information. Thus, our primary responsibility was to optimize the extraction process and streamline data to their storage.\n",
      "First of all, we had to start by setting up the required connectors. We integrated our ETL solution and streamlined marketing data from Facebook Ads, Google Ads, Snapchat, TikTok Ads, and other marketing connectors. Improvado’s data extraction module automated manual API requests and allowed Hyperconnect’s developers to focus on higher priority tasks. Then, we had to handle data grouping, normalization, and arrangement in the storage. In the last phase, we streamlined analysis-ready data to Google Data Studio. The Hyperconnect team required hourly data updates to stay on top of insights and supply their marketing dashboards with fresh information. With Improvado’s advanced scheduling feature, we managed to fulfill our partner’s requirements and deliver frequent updates. Since Hyperconnect wanted to track ROAS, CPA, and a vast range of custom metrics, we provided their team with proper marketing reports. At the beginning of the implementation, we provided the company with 20+ marketing reports. Later on, Hyperconnect’s team identified  which report options suited them the most, then reduced the volume of information extracted from data sources and loaded it to visualization tools. Our team worked closely with the Hyperconnect team throughout the integration process to stay on top of  their requirements and identify potential points of improvement.\n",
      "Our solution helped Hyperconnect improve their marketing performance indicators and reduce employees' time on routine data operations. Now, our client has access to more granular data and makes informed decisions faster. According to Hyperconnect, their marketing team now concentrates on marketing campaign optimization rather than data integration tasks. Furthermore, our ETL system provided employees across the company with easy access to all marketing reports. Sales managers, stakeholders, business analysts, and other competent employees can now analyze the company’s marketing efforts whenever they need to and instantly draw up marketing reports. Our advanced data extraction system has helped the company’s analysts find previously overlooked data and more precisely predict the outcomes of marketing strategies.  We’ve also provided Hyperconnect with more flexibility. Improvado is a scalable system, so when our client needs to add new marketing sources, we’ll be able to do it in a matter of seconds.\n",
      "Hyperconnect didn’t consider any solutions other than Improvado. Having found many positive reviews, the company’s HQ contacted us to solve their data extraction issues. During this collaboration, they complimented the high service level and the quality of our product. Hyperconnect described Improvado as “A Swiss army knife for marketing data.” Here’s what our client says about Improvado's influence on their marketing processes: “I believe we can now concentrate on more important tasks, such as optimization of the marketing campaigns, rather than data integration tasks.” We’ve helped Hyperconnect to significantly reduce their efforts on manual data operations and shortened the process of extracting new data.\n"
     ]
    }
   ],
   "source": [
    "docs = dataset['train'][0]\n",
    "for i in range(len(dataset['train'])):\n",
    "    for doc in dataset['train'][i].values():\n",
    "        print (doc)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def segment_documents(docs, max_doc_length=450):\n",
    "    # List containing full and segmented docs\n",
    "    segmented_docs = []\n",
    "    for i in range(len(dataset['train'])):\n",
    "        for doc in dataset['train'][i].values():\n",
    "#         for doc in docs:\n",
    "        # Split document by spaces to obtain a word count that roughly approximates the token count\n",
    "            split_to_words = doc.split(\" \")\n",
    "\n",
    "            # If the document is longer than our maximum length, split it up into smaller segments and add them to the list \n",
    "            if len(split_to_words) > max_doc_length:\n",
    "                for doc_segment in range(0, len(split_to_words), max_doc_length):\n",
    "                    segmented_docs.append( \" \".join(split_to_words[doc_segment:doc_segment + max_doc_length]))\n",
    "\n",
    "            # If the document is shorter than our maximum length, add it to the list\n",
    "            else:\n",
    "                segmented_docs.append(doc)\n",
    "\n",
    "    return segmented_docs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "\n",
    "def get_top_k_articles(query, docs, k=2):\n",
    "\n",
    "    # Initialize a vectorizer that removes English stop words\n",
    "    vectorizer = TfidfVectorizer(analyzer=\"word\", stop_words='english')\n",
    "\n",
    "    # Create a corpus of query and documents and convert to TFIDF vectors\n",
    "    query_and_docs = [query] + docs\n",
    "    matrix = vectorizer.fit_transform(query_and_docs)\n",
    "\n",
    "    # Holds our cosine similarity scores\n",
    "    scores = []\n",
    "\n",
    "    # The first vector is our query text, so compute the similarity of our query against all document vectors\n",
    "    for i in range(1, len(query_and_docs)):\n",
    "        scores.append(cosine_similarity(matrix[0], matrix[i])[0][0])\n",
    "\n",
    "    # Sort list of scores and return the top k highest scoring documents\n",
    "    sorted_list = sorted(enumerate(scores), key=lambda x: x[1], reverse=True)\n",
    "    top_doc_indices = [x[0] for x in sorted_list[:k]]\n",
    "    top_docs = [docs[x] for x in top_doc_indices]\n",
    "  \n",
    "    return top_docs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers.models.bert import BertTokenizer, BertForQuestionAnswering\n",
    "\n",
    "model = BertForQuestionAnswering.from_pretrained('bert-large-uncased-whole-word-masking-finetuned-squad')\n",
    "tokenizer = BertTokenizer.from_pretrained('bert-large-uncased-whole-word-masking-finetuned-squad')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def answer_question(question, answer_text):\n",
    "\n",
    "\tinput_ids = tokenizer.encode(question, answer_text, max_length=512)\n",
    "\t\n",
    "\t# ======== Set Segment IDs ========\n",
    "\t# Search the input_ids for the first instance of the `[SEP]` token.\n",
    "\tsep_index = input_ids.index(tokenizer.sep_token_id)\n",
    "\n",
    "\t# The number of segment A tokens includes the [SEP] token istelf.\n",
    "\tnum_seg_a = sep_index + 1\n",
    "\n",
    "\t# The remainder are segment B.\n",
    "\tnum_seg_b = len(input_ids) - num_seg_a\n",
    "\n",
    "\t# Construct the list of 0s and 1s.\n",
    "\tsegment_ids = [0]*num_seg_a + [1]*num_seg_b\n",
    "\n",
    "\t# There should be a segment_id for every input token.\n",
    "\tassert len(segment_ids) == len(input_ids)\n",
    "\n",
    "\toutputs = model(torch.tensor([input_ids]), token_type_ids=torch.tensor([segment_ids]), return_dict=True) \n",
    "\n",
    "\tstart_scores = outputs.start_logits\n",
    "\tend_scores = outputs.end_logits\n",
    "\n",
    "    # ======== Reconstruct Answer ========\n",
    "\t# Find the tokens with the highest `start` and `end` scores.\n",
    "\tanswer_start = torch.argmax(start_scores)\n",
    "\tanswer_end = torch.argmax(end_scores)\n",
    "\n",
    "        \n",
    "\t# Get the string versions of the input tokens.\n",
    "\ttokens = tokenizer.convert_ids_to_tokens(input_ids)\n",
    "\n",
    "\t# Start with the first token.\n",
    "\tanswer = tokens[answer_start]\n",
    "\n",
    "\t# Select the remaining answer tokens and join them with whitespace.\n",
    "\tfor i in range(answer_start + 1, answer_end + 1):\n",
    "\t\t\n",
    "\t\t# If it's a subword token, then recombine it with the previous token.\n",
    "\t\tif tokens[i][0:2] == '##':\n",
    "\t\t\tanswer += tokens[i][2:]\n",
    "\t\t\n",
    "\t\t# Otherwise, add a space then the token.\n",
    "\t\telse:\n",
    "\t\t\tanswer += ' ' + tokens[i]\n",
    "\n",
    "\tprint('Answer: \"' + answer + '\" with ' + f'{answer_end}' + '% confidence' )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Answer: \"employees across the company\" with 84% confidence\n",
      "Reference Document:  Our solution helped Hyperconnect improve their marketing performance indicators and reduce employees' time on routine data operations. Now, our client has access to more granular data and makes informed decisions faster. According to Hyperconnect, their marketing team now concentrates on marketing campaign optimization rather than data integration tasks. Furthermore, our ETL system provided employees across the company with easy access to all marketing reports. Sales managers, stakeholders, business analysts, and other competent employees can now analyze the company’s marketing efforts whenever they need to and instantly draw up marketing reports. Our advanced data extraction system has helped the company’s analysts find previously overlooked data and more precisely predict the outcomes of marketing strategies.  We’ve also provided Hyperconnect with more flexibility. Improvado is a scalable system, so when our client needs to add new marketing sources, we’ll be able to do it in a matter of seconds.\n",
      "Answer: \"400\" with 44% confidence\n",
      "Reference Document:  Hyperconnect aims to empower each person with an ability to connect and keep in touch with others. It’s a mid-sized company with around 400 employees, headquartered in Seoul, South Korea. As a global social platform, the company provides video and AI-powered software that helps users communicate in real time.\n",
      "Hyperconnect was dealing with fragmented data across different marketing channels. To extract this data, the company allocated developers to write lines of code that acted as marketing connectors. This process took too much time and led to  excessive use of the company’s resources. Furthermore, the gathered data was not normalized, which prevented marketing analysts from making informed decisions. Eventually, Hyperconnect contacted Improvado to optimize the data extraction process.\n",
      "Answer: \"hq\" with 11% confidence\n",
      "Reference Document:  Hyperconnect didn’t consider any solutions other than Improvado. Having found many positive reviews, the company’s HQ contacted us to solve their data extraction issues. During this collaboration, they complimented the high service level and the quality of our product. Hyperconnect described Improvado as “A Swiss army knife for marketing data.” Here’s what our client says about Improvado's influence on their marketing processes: “I believe we can now concentrate on more important tasks, such as optimization of the marketing campaigns, rather than data integration tasks.” We’ve helped Hyperconnect to significantly reduce their efforts on manual data operations and shortened the process of extracting new data.\n",
      "------------------------------------\n"
     ]
    }
   ],
   "source": [
    "# Enter our query here\n",
    "import torch \n",
    "query = \"How many employees in Hyperconnect company?\"\n",
    "#query = \"What else does the bassist for Death From Above play?\"\n",
    "#query = \"What projects is Jesse Keeler involved in?\"\n",
    "\n",
    "# Segment our documents\n",
    "segmented_docs = segment_documents(dataset['train'], 450)\n",
    "\n",
    "# Retrieve the top k most relevant documents to the query\n",
    "candidate_docs = get_top_k_articles(query, segmented_docs, 3)\n",
    "\n",
    "# Return the likeliest answers from each of our top k most relevant documents in descending order\n",
    "for i in candidate_docs:\n",
    "    answer_question(query, i)\n",
    "    print (\"Reference Document: \", i)\n",
    "    \n",
    "print(\"------------------------------------\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Answer: \"seoul , south korea\" with 55% confidence\n",
      "Reference Document:  Hyperconnect aims to empower each person with an ability to connect and keep in touch with others. It’s a mid-sized company with around 400 employees, headquartered in Seoul, South Korea. As a global social platform, the company provides video and AI-powered software that helps users communicate in real time.\n",
      "Hyperconnect was dealing with fragmented data across different marketing channels. To extract this data, the company allocated developers to write lines of code that acted as marketing connectors. This process took too much time and led to  excessive use of the company’s resources. Furthermore, the gathered data was not normalized, which prevented marketing analysts from making informed decisions. Eventually, Hyperconnect contacted Improvado to optimize the data extraction process.\n",
      "Answer: \"[SEP]\" with 14% confidence\n",
      "Reference Document:  Our solution helped Hyperconnect improve their marketing performance indicators and reduce employees' time on routine data operations. Now, our client has access to more granular data and makes informed decisions faster. According to Hyperconnect, their marketing team now concentrates on marketing campaign optimization rather than data integration tasks. Furthermore, our ETL system provided employees across the company with easy access to all marketing reports. Sales managers, stakeholders, business analysts, and other competent employees can now analyze the company’s marketing efforts whenever they need to and instantly draw up marketing reports. Our advanced data extraction system has helped the company’s analysts find previously overlooked data and more precisely predict the outcomes of marketing strategies.  We’ve also provided Hyperconnect with more flexibility. Improvado is a scalable system, so when our client needs to add new marketing sources, we’ll be able to do it in a matter of seconds.\n",
      "Answer: \"hq\" with 41% confidence\n",
      "Reference Document:  Hyperconnect didn’t consider any solutions other than Improvado. Having found many positive reviews, the company’s HQ contacted us to solve their data extraction issues. During this collaboration, they complimented the high service level and the quality of our product. Hyperconnect described Improvado as “A Swiss army knife for marketing data.” Here’s what our client says about Improvado's influence on their marketing processes: “I believe we can now concentrate on more important tasks, such as optimization of the marketing campaigns, rather than data integration tasks.” We’ve helped Hyperconnect to significantly reduce their efforts on manual data operations and shortened the process of extracting new data.\n"
     ]
    }
   ],
   "source": [
    "# Enter our query here\n",
    "import torch \n",
    "# query = \"How many employees in Hyperconnect company?\"\n",
    "query = \"Where is located headquater of Hyperconnect company?\"\n",
    "#query = \"What projects is Jesse Keeler involved in?\"\n",
    "\n",
    "# Segment our documents\n",
    "segmented_docs = segment_documents(dataset['train'], 450)\n",
    "\n",
    "# Retrieve the top k most relevant documents to the query\n",
    "candidate_docs = get_top_k_articles(query, segmented_docs, 3)\n",
    "\n",
    "# Return the likeliest answers from each of our top k most relevant documents in descending order\n",
    "for i in candidate_docs:\n",
    "    answer_question(query, i)\n",
    "    print (\"Reference Document: \", i)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
